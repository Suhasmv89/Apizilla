name: APIZilla - Custom GPT Swagger Validation

on:
  pull_request:
    paths:
      - '**/openapi*.yaml'
      - '**/openapi*.yml'
      - '**/swagger*.json'

jobs:
  gpt-review:
    runs-on: ubuntu-latest
    env:
      # optional default model id; replace with your actual Custom GPT ID
      CUSTOM_GPT_ID: gpt://g-68c8b6dae1308191a9a5d3608c757fe0-apizilla
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: find_spec
        id: find_spec
        run: |
          # look for common OpenAPI / Swagger filenames in the PR checkout
          echo "Searching for OpenAPI/Swagger files..."
          # find will output paths if found; else it returns non-zero (but we ignore exit code)
          found=$(git ls-files | grep -E '(^|/)(openapi.*\.ya?ml|swagger.*\.json)$' || true)
          if [ -n "$found" ]; then
            echo "Found spec file(s):"
            echo "$found"
            # write output via GITHUB_OUTPUT (recommended over set-output)
            echo "no_spec=0" >> "$GITHUB_OUTPUT"
            # optionally output the file list
            echo "spec_files<<EOF" >> "$GITHUB_OUTPUT"
            echo "$found" >> "$GITHUB_OUTPUT"
            echo "EOF" >> "$GITHUB_OUTPUT"
          else
            echo "No OpenAPI/Swagger files found."
            echo "no_spec=1" >> "$GITHUB_OUTPUT"
          fi

      - name: Prepare validation payload
        id: prepare_payload
        run: |
          # build JSON payload for the custom GPT call
          # customize SYSTEM_PROMPT and USER_PROMPT to your needs
          SYSTEM_PROMPT='You are APIZilla â€” validate the provided OpenAPI/Swagger spec and return a JSON report with "status","issues" and "fix_suggestions".'
          # you could pass the list of spec files as context, or upload the spec content (below is a simple usage)
          if [ "${{ steps.find_spec.outputs.no_spec }}" = "0" ]; then
            # take first spec file found (spec_files output contains newline separated)
            specfile=$(echo "${{ steps.find_spec.outputs.spec_files }}" | head -n1)
            spec_content=$(sed -e 's/\\/\\\\/g' -e 's/"/\\"/g' "$specfile")
            USER_PROMPT="Validate this OpenAPI/Swagger spec: $(basename "$specfile"). Return JSON only. Spec content below:\n\n$spec_content"
          else
            USER_PROMPT="No spec file found in the PR. Please check the repository. Return JSON indicating missing spec."
          fi

          # create payload file
          jq -n --arg sys "$SYSTEM_PROMPT" --arg usr "$USER_PROMPT" '{
            model: env.CUSTOM_GPT_ID,
            messages: [
              {role:"system", content:$sys},
              {role:"user", content:$usr}
            ],
            max_tokens: 1500,
            temperature: 0
          }' > validation_payload.json

          echo "payload_path=validation_payload.json" >> "$GITHUB_OUTPUT"

      - name: Call Custom GPT for Validation
        id: call_gpt
        if: success()                      # runs on success of previous steps (no extra condition)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          # If your Custom GPT uses a different host/endpoint, set it here
          OPENAI_API_BASE: https://api.openai.com/v1
        run: |
          set -e
          payload_file="${{ steps.prepare_payload.outputs.payload_path }}"
          echo "Calling Custom GPT with payload: $payload_file"

          # Example curl call to Chat Completions or your Custom GPT endpoint.
          # Replace the path or payload format if your Custom GPT requires a different API shape.
          # Here we call the OpenAI chat completions endpoint with the 'model' field set to your custom GPT id.
          response=$(curl -sS -X POST \
            "${OPENAI_API_BASE}/chat/completions" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${OPENAI_API_KEY}" \
            --data @"$payload_file")

          echo "Raw response:"
          echo "$response"

          # Save response for later steps or artifact upload
          echo "$response" > gpt_validation_response.json
          # Optionally set an output (be mindful of size)
          echo "gpt_response_path=gpt_validation_response.json" >> "$GITHUB_OUTPUT"

      - name: Upload validation result artifact
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: apizilla-validation
          path: |
            gpt_validation_response.json
            validation_payload.json

      # optional: fail the job based on validation severity by parsing the response
      - name: Fail on critical issues (optional)
        if: success()
        run: |
          # Example assumes the GPT returns JSON like: {"status":"failed","issues":[{"severity":"critical","msg":"..."}]}
          if jq -e '.issues[] | select(.severity=="critical")' gpt_validation_response.json >/dev/null 2>&1; then
            echo "Critical issues found in API spec. Failing the job."
            jq '.issues | map(select(.severity=="critical"))' gpt_validation_response.json
            exit 1
          else
            echo "No critical issues reported."
          fi
